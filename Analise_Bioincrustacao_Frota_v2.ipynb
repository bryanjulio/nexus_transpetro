{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wTiyIEWmm6Nf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-hX5sAo257j"
      },
      "source": [
        "# Predi√ß√£o de Bioincrusta√ß√£o Nexus\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87ea7944",
        "outputId": "5b99a1f6-ca1b-451e-80d4-4b59a461a6b6"
      },
      "source": [
        "# @title\n",
        "import pkg_resources\n",
        "import sys\n",
        "\n",
        "def create_requirements_file(filename=\"requirements.txt\"):\n",
        "    # List of libraries explicitly used in the notebook\n",
        "    explicit_dependencies = [\n",
        "        'pandas',\n",
        "        'numpy',\n",
        "        'matplotlib',\n",
        "        'seaborn',\n",
        "        'tqdm',\n",
        "        'scikit-learn',\n",
        "        'xgboost',\n",
        "        'lightgbm',\n",
        "        'gdown', # Included if used for data download\n",
        "        'openpyxl' # For reading/writing excel files, if applicable\n",
        "    ]\n",
        "\n",
        "    # Get all installed packages\n",
        "    installed_packages = {p.project_name.lower(): p for p in pkg_resources.working_set}\n",
        "\n",
        "    reqs = []\n",
        "    for dep_name in explicit_dependencies:\n",
        "        try:\n",
        "            # Try to get the package, handling case-insensitivity\n",
        "            package = installed_packages.get(dep_name.lower())\n",
        "            if package:\n",
        "                reqs.append(f\"{package.project_name}=={package.version}\")\n",
        "            else:\n",
        "                print(f\"Warning: '{dep_name}' specified but not found in environment. Skipping.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing '{dep_name}': {e}. Skipping.\")\n",
        "\n",
        "    with open(filename, \"w\") as f:\n",
        "        for r in sorted(reqs):\n",
        "            f.write(r + \"\\n\")\n",
        "    print(f\"Generated '{filename}' with {len(reqs)} key dependencies.\")\n",
        "    print(\"Please review the file and add any missing dependencies or remove unnecessary ones.\")\n",
        "\n",
        "create_requirements_file()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 'requirements.txt' with 10 key dependencies.\n",
            "Please review the file and add any missing dependencies or remove unnecessary ones.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1f6d2c85",
        "outputId": "0a14a03b-7028-40fe-cd9f-9979e8ff5848"
      },
      "source": [
        "# @title\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown==5.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (5.2.0)\n",
            "Requirement already satisfied: lightgbm==4.6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (4.6.0)\n",
            "Requirement already satisfied: matplotlib==3.10.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (3.10.0)\n",
            "Requirement already satisfied: numpy==2.0.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (2.0.2)\n",
            "Requirement already satisfied: openpyxl==3.1.5 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (3.1.5)\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn==1.6.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (1.6.1)\n",
            "Requirement already satisfied: seaborn==0.13.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (0.13.2)\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (4.67.1)\n",
            "Requirement already satisfied: xgboost==3.1.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (3.1.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown==5.2.0->-r requirements.txt (line 1)) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown==5.2.0->-r requirements.txt (line 1)) (3.20.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown==5.2.0->-r requirements.txt (line 1)) (2.32.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lightgbm==4.6.0->-r requirements.txt (line 2)) (1.16.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0->-r requirements.txt (line 3)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0->-r requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0->-r requirements.txt (line 3)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0->-r requirements.txt (line 3)) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0->-r requirements.txt (line 3)) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0->-r requirements.txt (line 3)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0->-r requirements.txt (line 3)) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0->-r requirements.txt (line 3)) (2.9.0.post0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl==3.1.5->-r requirements.txt (line 5)) (2.0.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2->-r requirements.txt (line 6)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2->-r requirements.txt (line 6)) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1->-r requirements.txt (line 7)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1->-r requirements.txt (line 7)) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost==3.1.2->-r requirements.txt (line 10)) (2.27.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib==3.10.0->-r requirements.txt (line 3)) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown==5.2.0->-r requirements.txt (line 1)) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown==5.2.0->-r requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown==5.2.0->-r requirements.txt (line 1)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown==5.2.0->-r requirements.txt (line 1)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown==5.2.0->-r requirements.txt (line 1)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown==5.2.0->-r requirements.txt (line 1)) (2025.11.12)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown==5.2.0->-r requirements.txt (line 1)) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tJWrSkd3257j",
        "outputId": "0bf8889f-8c18-4529-8c07-ab9c69a70f33",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bibliotecas importadas com sucesso!\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "\"\"\"\n",
        "Predi√ß√£o de Bioincrusta√ß√£o - An√°lise Avan√ßada de Fouling\n",
        "\n",
        "\n",
        "1. Features de tempo ocioso (idle time)\n",
        "2. Features de velocidade de risco\n",
        "3. Progress√£o temporal da bioincrusta√ß√£o\n",
        "4. Valida√ß√£o temporal (n√£o aleat√≥ria)\n",
        "5. Modelo ensemble (XGBoost, LightGBM, RF, GB)\n",
        "6. Target baseado em Fouling Rating IMO (0-4)\n",
        "7. An√°lise de cen√°rios futuros\n",
        "8. Impacto econ√¥mico realista (5-25% penalty)\n",
        "9. An√°lise individual por navio da frota\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import zipfile\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ML imports\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import pickle\n",
        "\n",
        "# Configura√ß√µes\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2910bbde",
        "outputId": "f681142c-7c80-4133-eaed-ed3e1d7b1bb0",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 'requirements.txt' with 10 key dependencies.\n",
            "Please review the file and add any missing dependencies or remove unnecessary ones.\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "import pkg_resources\n",
        "import sys\n",
        "\n",
        "def create_requirements_file(filename=\"requirements.txt\"):\n",
        "    # List of libraries explicitly used in the notebook\n",
        "    explicit_dependencies = [\n",
        "        'pandas',\n",
        "        'numpy',\n",
        "        'matplotlib',\n",
        "        'seaborn',\n",
        "        'tqdm',\n",
        "        'scikit-learn',\n",
        "        'xgboost',\n",
        "        'lightgbm',\n",
        "        'gdown', # Included if used for data download\n",
        "        'openpyxl' # For reading/writing excel files, if applicable\n",
        "    ]\n",
        "\n",
        "    # Get all installed packages\n",
        "    installed_packages = {p.project_name.lower(): p for p in pkg_resources.working_set}\n",
        "\n",
        "    reqs = []\n",
        "    for dep_name in explicit_dependencies:\n",
        "        try:\n",
        "            # Try to get the package, handling case-insensitivity\n",
        "            package = installed_packages.get(dep_name.lower())\n",
        "            if package:\n",
        "                reqs.append(f\"{package.project_name}=={package.version}\")\n",
        "            else:\n",
        "                print(f\"Warning: '{dep_name}' specified but not found in environment. Skipping.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing '{dep_name}': {e}. Skipping.\")\n",
        "\n",
        "    # Additionally, check for packages in the current environment that might be implied\n",
        "    # This part is more general and might pick up extras, but ensures coverage\n",
        "    # For a more precise list, manually curate explicit_dependencies.\n",
        "\n",
        "    # Get all direct imports in the current kernel, though this is harder to automate perfectly.\n",
        "    # For simplicity, we stick to explicit_dependencies here unless a more complex introspection is needed.\n",
        "\n",
        "    with open(filename, \"w\") as f:\n",
        "        for r in sorted(reqs):\n",
        "            f.write(r + \"\\n\")\n",
        "    print(f\"Generated '{filename}' with {len(reqs)} key dependencies.\")\n",
        "    print(\"Please review the file and add any missing dependencies or remove unnecessary ones.\")\n",
        "\n",
        "create_requirements_file()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCy2vjmv257k"
      },
      "source": [
        "## 1.5. DOWNLOAD DOS DADOS DO GOOGLE DRIVE (OPCIONAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3en5nZfr257l",
        "outputId": "d2746243-f0ed-4b8c-a99d-ce9617ae8895",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Baixando dados do Google Drive...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder 1oSLdQSsW0GpFgGoRZF12zFxFNs72P0sH Mais Dados\n",
            "Processing file 1IzjTamdx1iq2MTi2VkrO6lAYF3s_uSIL AIS_NAVIO TESTE 2 1.csv\n",
            "Processing file 1rP-GH7HLBMLS-DAQ6st9689wDtgY3YMe AIS_NAVIO TESTE 3 1.csv\n",
            "Processing file 17PjAApZZCyk_2epri9x-CRyD-1BoUPor Consumo_Validacao 1.CSV\n",
            "Processing file 1MoajA9gX0OHrEFdGyBRDh5DSXg0H7X-S Dados navios Valida√ß√£o 1.xlsx\n",
            "Processing file 1xksJEcxznpN_anRavD_0x-4c2t8hQYRN Eventos_Validacao 1.CSV\n",
            "Processing file 1BNU0xrGH54VYviSBNKGZG5Xkya4CTf0C RESULTADO Valida√ß√£o 1.xlsx\n",
            "Processing file 17-kgs1RS52wenFcfpHr2ljbkq-4TG2Cf Dados AIS frota TP.zip\n",
            "Processing file 1_CTM1V1PFN2guPl2ipW-VjdM80i8i7Ll Dados navios Hackathon.xlsx\n",
            "Processing file 1L-iN3artAlSB3hqC6pQsX58z_9HKbmVw Dicion√°rios de Dados.xlsx\n",
            "Processing file 1YYp8B3finjq-p53MURKGyPXUQt26ZIuf Manual do Participante.pdf\n",
            "Processing file 1ikd9AFsF18LZTAW8mRofQhVWaXzN98ek Relatorios IWS.xlsx\n",
            "Processing file 1XUPl_mDEVjtlM6g19Q-oyaRWB9Gn6a0M ResultadoQueryConsumo.csv\n",
            "Processing file 1S4iA70w2SapFOrNz1UAAWuWL1xBOD-7D ResultadoQueryEventos.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1IzjTamdx1iq2MTi2VkrO6lAYF3s_uSIL\n",
            "To: /content/Hackathon Transpetro/Mais Dados/AIS_NAVIO TESTE 2 1.csv\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 988k/988k [00:00<00:00, 12.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rP-GH7HLBMLS-DAQ6st9689wDtgY3YMe\n",
            "To: /content/Hackathon Transpetro/Mais Dados/AIS_NAVIO TESTE 3 1.csv\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.22M/1.22M [00:00<00:00, 13.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17PjAApZZCyk_2epri9x-CRyD-1BoUPor\n",
            "To: /content/Hackathon Transpetro/Mais Dados/Consumo_Validacao 1.CSV\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 191k/191k [00:00<00:00, 4.92MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1MoajA9gX0OHrEFdGyBRDh5DSXg0H7X-S\n",
            "To: /content/Hackathon Transpetro/Mais Dados/Dados navios Valida√ß√£o 1.xlsx\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12.9k/12.9k [00:00<00:00, 6.13MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xksJEcxznpN_anRavD_0x-4c2t8hQYRN\n",
            "To: /content/Hackathon Transpetro/Mais Dados/Eventos_Validacao 1.CSV\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 586k/586k [00:00<00:00, 4.83MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1BNU0xrGH54VYviSBNKGZG5Xkya4CTf0C\n",
            "To: /content/Hackathon Transpetro/Mais Dados/RESULTADO Valida√ß√£o 1.xlsx\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10.5k/10.5k [00:00<00:00, 22.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17-kgs1RS52wenFcfpHr2ljbkq-4TG2Cf\n",
            "To: /content/Hackathon Transpetro/Dados AIS frota TP.zip\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.47M/6.47M [00:00<00:00, 29.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_CTM1V1PFN2guPl2ipW-VjdM80i8i7Ll\n",
            "To: /content/Hackathon Transpetro/Dados navios Hackathon.xlsx\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16.4k/16.4k [00:00<00:00, 2.07MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1L-iN3artAlSB3hqC6pQsX58z_9HKbmVw\n",
            "To: /content/Hackathon Transpetro/Dicion√°rios de Dados.xlsx\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19.0k/19.0k [00:00<00:00, 18.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1YYp8B3finjq-p53MURKGyPXUQt26ZIuf\n",
            "To: /content/Hackathon Transpetro/Manual do Participante.pdf\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.90M/3.90M [00:00<00:00, 30.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ikd9AFsF18LZTAW8mRofQhVWaXzN98ek\n",
            "To: /content/Hackathon Transpetro/Relatorios IWS.xlsx\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15.8k/15.8k [00:00<00:00, 28.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1XUPl_mDEVjtlM6g19Q-oyaRWB9Gn6a0M\n",
            "To: /content/Hackathon Transpetro/ResultadoQueryConsumo.csv\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.39M/2.39M [00:00<00:00, 20.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1S4iA70w2SapFOrNz1UAAWuWL1xBOD-7D\n",
            "To: /content/Hackathon Transpetro/ResultadoQueryEventos.csv\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8.35M/8.35M [00:00<00:00, 55.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Dados baixados com sucesso!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Download completed\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "\n",
        "# Se os dados n√£o existirem localmente, baixar do Google Drive\n",
        "DOWNLOAD_FROM_DRIVE = True  # Altere para True para baixar do Drive\n",
        "\n",
        "if DOWNLOAD_FROM_DRIVE:\n",
        "    try:\n",
        "        import gdown\n",
        "        print(\"\\n Baixando dados do Google Drive...\")\n",
        "\n",
        "        folder_url = \"https://drive.google.com/drive/folders/1NJrDlremklekCO1NR4Ltm43DZBOCKdW6\"\n",
        "        gdown.download_folder(folder_url, quiet=False, use_cookies=False, output=\"Hackathon Transpetro\")\n",
        "\n",
        "        print(\" Dados baixados com sucesso!\")\n",
        "        BASE_PATH = \"Dados Hackathon Transpetro/\"\n",
        "    except ImportError:\n",
        "        print(\" gdown n√£o instalado. Execute: pip install gdown\")\n",
        "        print(\"   Usando dados locais...\")\n",
        "        BASE_PATH = \"\"\n",
        "    except Exception as e:\n",
        "        print(f\" Erro ao baixar do Drive: {e}\")\n",
        "        print(\"   Usando dados locais...\")\n",
        "        BASE_PATH = \"\"\n",
        "else:\n",
        "    BASE_PATH = \"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eemxl3Y257l"
      },
      "source": [
        "## 2. CARREGAMENTO DOS DADOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD0qylI5257l",
        "outputId": "7248b106-c09f-4b73-e68a-781bbc2c93e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìÇ Carregando dados de: Hackathon Transpetro/\n",
            " Eventos: (50904, 22)\n",
            " Consumo: (87737, 3)\n",
            " Navios: (21, 8)\n",
            " IWS: (29, 14)\n",
            "‚úÖ AIS carregado de Hackathon Transpetro/Dados AIS frota TP.zip: (415724, 7)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Se n√£o foi definido BASE_PATH no download, usar caminho local\n",
        "if 'BASE_PATH' not in locals():\n",
        "    BASE_PATH = \"/Users/bryan/Documents/Hackathon_transpetro/\"\n",
        "\n",
        "# Tentar m√∫ltiplos caminhos poss√≠veis\n",
        "data_paths = [\n",
        "    BASE_PATH,\n",
        "    \"Hackathon Transpetro/\",\n",
        "    \"/content/Hackathon Transpetro/\",  # Google Colab\n",
        "    \"\"  # Diret√≥rio atual\n",
        "]\n",
        "\n",
        "# Encontrar caminho v√°lido\n",
        "valid_path = None\n",
        "for path in data_paths:\n",
        "    if os.path.exists(f\"{path}ResultadoQueryEventos.csv\"):\n",
        "        valid_path = path\n",
        "        break\n",
        "\n",
        "if valid_path is None:\n",
        "    print(\"‚ö†Ô∏è Dados n√£o encontrados. Configure DOWNLOAD_FROM_DRIVE=True ou ajuste BASE_PATH\")\n",
        "    exit(1)\n",
        "\n",
        "BASE_PATH = valid_path\n",
        "print(f\"\\nüìÇ Carregando dados de: {BASE_PATH}\")\n",
        "\n",
        "df_eventos = pd.read_csv(f\"{BASE_PATH}ResultadoQueryEventos.csv\")\n",
        "df_consumo = pd.read_csv(f\"{BASE_PATH}ResultadoQueryConsumo.csv\")\n",
        "df_navios = pd.read_excel(f\"{BASE_PATH}Dados navios Hackathon.xlsx\")\n",
        "df_iws = pd.read_excel(f\"{BASE_PATH}Relatorios IWS.xlsx\")\n",
        "\n",
        "print(f\" Eventos: {df_eventos.shape}\")\n",
        "print(f\" Consumo: {df_consumo.shape}\")\n",
        "print(f\" Navios: {df_navios.shape}\")\n",
        "print(f\" IWS: {df_iws.shape}\")\n",
        "\n",
        "# Carregar AIS\n",
        "# Tentar m√∫ltiplos caminhos poss√≠veis\n",
        "ais_paths = [\n",
        "    f\"{BASE_PATH}Dados AIS frota TP\",  # Pasta descompactada\n",
        "    f\"{BASE_PATH}notebooks/Dados Hackathon Transpetro/Dados AIS frota TP.zip\",  # ZIP no notebooks\n",
        "    f\"{BASE_PATH}Dados AIS frota TP.zip\"  # ZIP na raiz\n",
        "]\n",
        "\n",
        "df_ais = pd.DataFrame()\n",
        "ais_loaded = False\n",
        "\n",
        "for ais_path in ais_paths:\n",
        "    if os.path.exists(ais_path):\n",
        "        if ais_path.endswith('.zip'):\n",
        "            # √â um ZIP, extrair\n",
        "            extract_folder = \"dados_ais_temp\"\n",
        "            with zipfile.ZipFile(ais_path, \"r\") as z:\n",
        "                z.extractall(extract_folder)\n",
        "            csv_folder = os.path.join(extract_folder, \"Dados AIS frota TP\")\n",
        "        else:\n",
        "            # J√° √© uma pasta\n",
        "            csv_folder = ais_path\n",
        "\n",
        "        # Ler CSVs\n",
        "        all_dfs = []\n",
        "        for file_name in os.listdir(csv_folder):\n",
        "            if file_name.lower().endswith(\".csv\"):\n",
        "                file_path = os.path.join(csv_folder, file_name)\n",
        "                df = pd.read_csv(file_path)\n",
        "                df[\"ARQUIVO_ORIGEM\"] = file_name\n",
        "                all_dfs.append(df)\n",
        "\n",
        "        if all_dfs:\n",
        "            df_ais = pd.concat(all_dfs, ignore_index=True)\n",
        "            print(f\"‚úÖ AIS carregado de {ais_path}: {df_ais.shape}\")\n",
        "            ais_loaded = True\n",
        "            break\n",
        "\n",
        "if not ais_loaded:\n",
        "    print(\" Arquivo AIS n√£o encontrado em nenhum caminho\")\n",
        "    print(f\"   Tentou: {ais_paths}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLy5wxpW257m"
      },
      "source": [
        "## 2. PR√â-PROCESSAMENTO B√ÅSICO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6bBVVWn257m",
        "outputId": "f405d006-a55a-44b4-db0c-de1962e8e224",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Pr√©-processamento...\n",
            "Pr√©-processamento conclu√≠do!\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "\n",
        "print(\"\\n Pr√©-processamento...\")\n",
        "\n",
        "# Padronizar colunas\n",
        "df_eventos.columns = df_eventos.columns.str.strip()\n",
        "df_consumo.columns = df_consumo.columns.str.strip()\n",
        "df_navios.columns = df_navios.columns.str.strip()\n",
        "df_iws.columns = df_iws.columns.str.strip()\n",
        "\n",
        "# Parse datetimes\n",
        "for c in [\"startGMTDate\", \"endGMTDate\"]:\n",
        "    if c in df_eventos.columns:\n",
        "        df_eventos[c] = pd.to_datetime(df_eventos[c], errors='coerce')\n",
        "\n",
        "# Renomear SESSION_ID\n",
        "if \"SESSION_ID\" in df_consumo.columns:\n",
        "    df_consumo.rename(columns={\"SESSION_ID\": \"sessionId\"}, inplace=True)\n",
        "\n",
        "# Processar AIS\n",
        "if not df_ais.empty:\n",
        "    df_ais.columns = df_ais.columns.str.strip()\n",
        "\n",
        "    for cand in [\"DATAHORA\", \"DataHora\", \"datahora\", \"DATETIME\"]:\n",
        "        if cand in df_ais.columns:\n",
        "            df_ais['DATETIME'] = pd.to_datetime(df_ais[cand], errors='coerce')\n",
        "            break\n",
        "\n",
        "    for vcol in [\"VELOCIDADE\", \"speed\", \"SOG\", \"speedGps\"]:\n",
        "        if vcol in df_ais.columns:\n",
        "            df_ais['speed_kn'] = pd.to_numeric(df_ais[vcol], errors='coerce')\n",
        "            break\n",
        "\n",
        "    for cand in [\"NOME\", \"name\", \"ship\", \"shipName\", \"ARQUIVO_ORIGEM\"]:\n",
        "        if cand in df_ais.columns:\n",
        "            df_ais['shipName_ais'] = df_ais[cand].astype(str)\n",
        "            break\n",
        "\n",
        "print(\"Pr√©-processamento conclu√≠do!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgV_olYP257m"
      },
      "source": [
        "## 3. AGREGA√á√ÉO AIS POR EVENTO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMqrA8Pg257m",
        "outputId": "b9ea0d38-0f95-497e-e1f2-f3d2e1d93073",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Agregando dados AIS...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Agregando AIS: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50904/50904 [00:59<00:00, 854.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eventos com AIS: (8214, 17)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "\n",
        "def aggregate_ais_by_event(df_eventos, df_ais):\n",
        "    \"\"\"Agrega dados AIS para cada evento de navega√ß√£o\"\"\"\n",
        "    agg_rows = []\n",
        "\n",
        "    if df_eventos.empty or df_ais.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df_ais['shipName_ais_low'] = df_ais['shipName_ais'].str.lower().str.strip()\n",
        "    df_eventos['shipName_low'] = df_eventos['shipName'].astype(str).str.lower().str.strip()\n",
        "\n",
        "    ais_groups = {k: g for k, g in df_ais.groupby('shipName_ais_low')}\n",
        "\n",
        "    for idx, ev in tqdm(df_eventos.iterrows(), total=len(df_eventos), desc=\"Agregando AIS\"):\n",
        "        ship = str(ev.get('shipName_low', \"\")).strip()\n",
        "        sdt = ev.get('startGMTDate')\n",
        "        edt = ev.get('endGMTDate')\n",
        "\n",
        "        if ship == \"\" or pd.isna(sdt) or pd.isna(edt):\n",
        "            continue\n",
        "\n",
        "        ais_g = ais_groups.get(ship)\n",
        "        if ais_g is None:\n",
        "            candidates = [k for k in ais_groups.keys() if ship in k or k in ship]\n",
        "            ais_g = ais_groups.get(candidates[0]) if candidates else None\n",
        "\n",
        "        if ais_g is None:\n",
        "            continue\n",
        "\n",
        "        window = ais_g[(ais_g['DATETIME'] >= sdt) & (ais_g['DATETIME'] <= edt)]\n",
        "\n",
        "        if window.empty:\n",
        "            continue\n",
        "\n",
        "        speed_mean = window['speed_kn'].mean()\n",
        "        speed_std = window['speed_kn'].std()\n",
        "        speed_min = window['speed_kn'].min()\n",
        "        speed_max = window['speed_kn'].max()\n",
        "        frac_stop = (window['speed_kn'] < 1.5).mean()\n",
        "        frac_low_speed = (window['speed_kn'] < 5).mean()\n",
        "\n",
        "        lat_mean = pd.to_numeric(window.get('LATITUDE', window.get('latitude', pd.Series(np.nan))), errors='coerce').mean()\n",
        "        lon_mean = pd.to_numeric(window.get('LONGITUDE', window.get('longitude', pd.Series(np.nan))), errors='coerce').mean()\n",
        "\n",
        "        agg_rows.append({\n",
        "            'sessionId': ev.get('sessionId'),\n",
        "            'shipName': ev.get('shipName'),\n",
        "            'startGMTDate': sdt,\n",
        "            'endGMTDate': edt,\n",
        "            'duration_h': ev.get('duration'),\n",
        "            'distance': ev.get('distance'),\n",
        "            'beaufort': ev.get('beaufortScale'),\n",
        "            'seaCondition': ev.get('seaCondition'),\n",
        "            'displacement': ev.get('displacement'),\n",
        "            'speed_mean': speed_mean,\n",
        "            'speed_std': speed_std,\n",
        "            'speed_min': speed_min,\n",
        "            'speed_max': speed_max,\n",
        "            'frac_stop': frac_stop,\n",
        "            'frac_low_speed': frac_low_speed,\n",
        "            'lat_mean': lat_mean,\n",
        "            'lon_mean': lon_mean\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(agg_rows)\n",
        "\n",
        "print(\"\\n Agregando dados AIS...\")\n",
        "df_events_ais = aggregate_ais_by_event(df_eventos, df_ais)\n",
        "print(f\"Eventos com AIS: {df_events_ais.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzuxrPW7257n"
      },
      "source": [
        "## 4. FEATURES AVAN√áADAS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdCpvPau257n",
        "outputId": "4959af3b-a3df-4df7-9829-08e61f3b75c7",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features avan√ßadas criadas!\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "\n",
        "def create_advanced_features(df):\n",
        "    \"\"\"Cria features avan√ßadas baseadas em ci√™ncia de bioincrusta√ß√£o\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # 1. IDLE TIME FEATURES (CR√çTICO)\n",
        "    df['idle_time_ratio'] = df['frac_stop'].fillna(0)\n",
        "    df['idle_days'] = (df['duration_h'] * df['idle_time_ratio'] / 24).fillna(0)\n",
        "    df['low_speed_days'] = (df['duration_h'] * df['frac_low_speed'] / 24).fillna(0)\n",
        "\n",
        "    # 2. VELOCITY RISK SCORE (CR√çTICO)\n",
        "    def velocity_risk(speed):\n",
        "        if pd.isna(speed):\n",
        "            return 2\n",
        "        if speed < 5:\n",
        "            return 3  # Alto risco\n",
        "        elif speed < 10:\n",
        "            return 2  # Risco moderado\n",
        "        elif speed < 12:\n",
        "            return 1  # Baixo-moderado\n",
        "        else:\n",
        "            return 0  # Baixo risco\n",
        "\n",
        "    df['velocity_risk'] = df['speed_mean'].apply(velocity_risk)\n",
        "\n",
        "    # 3. OPERATIONAL PROFILE\n",
        "    df['operation_continuity'] = 1 - df['idle_time_ratio']\n",
        "\n",
        "    # 4. LOW SHEAR ZONES EXPOSURE\n",
        "    df['low_shear_exposure'] = df['idle_days'] * (df['velocity_risk'] + 1)\n",
        "\n",
        "    # 5. BIOGEOGRAPHIC REGION RISK\n",
        "    def get_biogeographic_region(lat):\n",
        "        if pd.isna(lat):\n",
        "            return 'Unknown'\n",
        "        if lat > -5:\n",
        "            return 'Norte'\n",
        "        elif lat > -15:\n",
        "            return 'Nordeste'\n",
        "        else:\n",
        "            return 'Sudeste-Sul'\n",
        "\n",
        "    df['bio_region'] = df['lat_mean'].apply(get_biogeographic_region)\n",
        "    region_risk = {'Norte': 3, 'Nordeste': 2, 'Sudeste-Sul': 1, 'Unknown': 1.5}\n",
        "    df['region_risk'] = df['bio_region'].map(region_risk)\n",
        "\n",
        "    # 6. TEMPERATURE PROXY\n",
        "    df['temp_proxy'] = df['lat_mean'].abs().fillna(15)\n",
        "    df['temp_risk'] = (15 - df['temp_proxy']).clip(0, 15) / 15\n",
        "\n",
        "    # 7. SPEED VARIABILITY\n",
        "    df['speed_variability'] = df['speed_std'] / (df['speed_mean'] + 1)\n",
        "\n",
        "    print(\"Features avan√ßadas criadas!\")\n",
        "    return df\n",
        "\n",
        "if not df_events_ais.empty:\n",
        "    df_events_ais = create_advanced_features(df_events_ais)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHFQsMWf257n"
      },
      "source": [
        "## 5. PROCESSAR IWS E CRIAR TARGET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UrM8eWZ257n",
        "outputId": "24023619-c026-4fa7-a74c-b3f3740f189c",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Processando IWS e criando target...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculando dias desde limpeza: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8214/8214 [00:08<00:00, 983.11it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fouling Rating target criado!\n",
            "Distribui√ß√£o:\n",
            "count    4639.000000\n",
            "mean        3.635577\n",
            "std         0.739599\n",
            "min         0.225000\n",
            "25%         3.725000\n",
            "50%         4.000000\n",
            "75%         4.000000\n",
            "max         4.000000\n",
            "Name: fouling_rating, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "\n",
        "def process_iws_data(df_iws, df_events_ais):\n",
        "    \"\"\"Processa dados de inspe√ß√£o e calcula dias desde √∫ltima limpeza\"\"\"\n",
        "    if df_iws.empty or df_events_ais.empty:\n",
        "        return df_events_ais\n",
        "\n",
        "    iw_cols = [c for c in df_iws.columns if 'data' in c.lower()]\n",
        "    possible_ship_cols = [c for c in df_iws.columns if 'embarca' in c.lower() or 'navio' in c.lower()]\n",
        "\n",
        "    if not iw_cols or not possible_ship_cols:\n",
        "        print(\"Colunas de data/navio n√£o encontradas no IWS\")\n",
        "        return df_events_ais\n",
        "\n",
        "    date_col = iw_cols[0]\n",
        "    ship_col = possible_ship_cols[0]\n",
        "\n",
        "    df_iws['date_iws'] = pd.to_datetime(df_iws[date_col], errors='coerce')\n",
        "    df_iws['ship_iws'] = df_iws[ship_col].astype(str).str.lower().str.strip()\n",
        "\n",
        "    median_interval = df_iws.groupby('ship_iws')['date_iws'].apply(\n",
        "        lambda g: g.sort_values().diff().dt.days.median()\n",
        "    ).rename('median_interval').reset_index()\n",
        "    median_interval['median_interval'].fillna(180, inplace=True)\n",
        "\n",
        "    def days_since_last_clean(row):\n",
        "        s = str(row['shipName']).lower().strip()\n",
        "        start = row['startGMTDate']\n",
        "        if pd.isna(start):\n",
        "            return np.nan, np.nan\n",
        "\n",
        "        cleans = df_iws[(df_iws['ship_iws'] == s) & (df_iws['date_iws'] <= start)]\n",
        "        if cleans.empty:\n",
        "            return np.nan, np.nan\n",
        "\n",
        "        last = cleans['date_iws'].max()\n",
        "        median = median_interval[median_interval['ship_iws'] == s]['median_interval']\n",
        "        median_val = median.values[0] if not median.empty else 180\n",
        "\n",
        "        return (start - last).days, median_val\n",
        "\n",
        "    days_med = []\n",
        "    median_list = []\n",
        "\n",
        "    for _, r in tqdm(df_events_ais.iterrows(), total=len(df_events_ais), desc=\"Calculando dias desde limpeza\"):\n",
        "        d, med = days_since_last_clean(r)\n",
        "        days_med.append(d)\n",
        "        median_list.append(med)\n",
        "\n",
        "    df_events_ais['days_since_clean'] = days_med\n",
        "    df_events_ais['median_interval'] = median_list\n",
        "\n",
        "    return df_events_ais\n",
        "\n",
        "def create_fouling_rating_target(df):\n",
        "    \"\"\"\n",
        "    Cria target baseado em Fouling Rating IMO (0-4)\n",
        "\n",
        "    Escala IMO MEPC.378(80):\n",
        "    0: Sem bioincrusta√ß√£o\n",
        "    1: Microincrusta√ß√£o (biofilme/limo)\n",
        "    2: Macroincrusta√ß√£o leve (1-15%)\n",
        "    3: Macroincrusta√ß√£o moderada (16-40%)\n",
        "    4: Macroincrusta√ß√£o pesada (41-100%)\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    def estimate_fouling_rating(row):\n",
        "        days = row.get('days_since_clean', np.nan)\n",
        "        velocity_risk = row.get('velocity_risk', 2)\n",
        "        idle_ratio = row.get('idle_time_ratio', 0)\n",
        "        temp_risk = row.get('temp_risk', 0.5)\n",
        "        region_risk = row.get('region_risk', 1.5)\n",
        "\n",
        "        if pd.isna(days):\n",
        "            return np.nan\n",
        "\n",
        "        # Base score por tempo\n",
        "        if days < 14:\n",
        "            base_score = 0.3\n",
        "        elif days < 42:\n",
        "            base_score = 1.2\n",
        "        elif days < 90:\n",
        "            base_score = 2.0\n",
        "        elif days < 180:\n",
        "            base_score = 3.0\n",
        "        else:\n",
        "            base_score = 3.8\n",
        "\n",
        "        # Modificadores\n",
        "        velocity_modifier = velocity_risk * 0.15\n",
        "        idle_modifier = idle_ratio * 0.4\n",
        "        temp_modifier = temp_risk * 0.2\n",
        "        region_modifier = (region_risk - 1.5) * 0.15\n",
        "\n",
        "        final_score = base_score + velocity_modifier + idle_modifier + temp_modifier + region_modifier\n",
        "\n",
        "        return np.clip(final_score, 0, 4)\n",
        "\n",
        "    df['fouling_rating'] = df.apply(estimate_fouling_rating, axis=1)\n",
        "\n",
        "    # Criar est√°gios\n",
        "    def get_fouling_stage(days):\n",
        "        if pd.isna(days):\n",
        "            return np.nan\n",
        "        if days < 14:\n",
        "            return 0\n",
        "        elif days < 42:\n",
        "            return 1\n",
        "        elif days < 90:\n",
        "            return 2\n",
        "        else:\n",
        "            return 3\n",
        "\n",
        "    df['fouling_stage'] = df['days_since_clean'].apply(get_fouling_stage)\n",
        "\n",
        "    # Labels categ√≥ricos\n",
        "    def get_fouling_label(rating):\n",
        "        if pd.isna(rating):\n",
        "            return np.nan\n",
        "        if rating < 1:\n",
        "            return 'clean'\n",
        "        elif rating < 2:\n",
        "            return 'light'\n",
        "        elif rating < 3:\n",
        "            return 'moderate'\n",
        "        else:\n",
        "            return 'heavy'\n",
        "\n",
        "    df['fouling_label'] = df['fouling_rating'].apply(get_fouling_label)\n",
        "\n",
        "    # Risk score combinado\n",
        "    df['biofouling_risk_score'] = (\n",
        "        0.4 * (df['days_since_clean'].fillna(90) / 180).clip(0, 1) +\n",
        "        0.25 * (df['velocity_risk'] / 3) +\n",
        "        0.2 * df['idle_time_ratio'] +\n",
        "        0.15 * df['temp_risk']\n",
        "    ).clip(0, 1)\n",
        "\n",
        "    print(\"Fouling Rating target criado!\")\n",
        "    print(f\"Distribui√ß√£o:\")\n",
        "    print(df['fouling_rating'].describe())\n",
        "\n",
        "    return df\n",
        "\n",
        "print(\"\\n Processando IWS e criando target...\")\n",
        "if not df_events_ais.empty:\n",
        "    df_events_ais = process_iws_data(df_iws, df_events_ais)\n",
        "    df_events_ais = create_fouling_rating_target(df_events_ais)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rinO6Sa0257n"
      },
      "source": [
        "## 6. MERGE COM CONSUMO E NAVIOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPXSFUx-257o",
        "outputId": "07b0c02d-b243-40e0-e47c-480ee27ad73a",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîó Merging dados...\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "\n",
        "print(\"\\nüîó Merging dados...\")\n",
        "if not df_events_ais.empty and 'sessionId' in df_consumo.columns:\n",
        "    df_cons_sum = df_consumo.groupby('sessionId', as_index=False)['CONSUMED_QUANTITY'].sum()\n",
        "    df_events_ais = df_events_ais.merge(df_cons_sum, on='sessionId', how='left')\n",
        "\n",
        "if not df_events_ais.empty and not df_navios.empty:\n",
        "    shipname_col = [c for c in df_navios.columns if 'nome' in c.lower() or 'name' in c.lower()]\n",
        "    if shipname_col:\n",
        "        snc = shipname_col[0]\n",
        "        df_navios['ship_nav_low'] = df_navios[snc].astype(str).str.lower().str.strip()\n",
        "        df_events_ais['ship_low'] = df_events_ais['shipName'].astype(str).str.lower().str.strip()\n",
        "        df_events_ais = df_events_ais.merge(df_navios, left_on='ship_low', right_on='ship_nav_low', how='left')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj8b64Uz257o"
      },
      "source": [
        "## 7. PREPARAR DATASET ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4K8sTdW257o",
        "outputId": "c6041251-750c-4f14-8f77-b13f5e7f4108",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Features dispon√≠veis: 26\n",
            "‚úÖ Dataset ML: (4639, 30)\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "\n",
        "features_v2 = [\n",
        "    'speed_mean', 'speed_std', 'speed_min', 'speed_max',\n",
        "    'duration_h', 'distance',\n",
        "    'frac_stop', 'frac_low_speed', 'idle_days', 'low_speed_days',\n",
        "    'velocity_risk', 'operation_continuity', 'speed_variability',\n",
        "    'low_shear_exposure', 'biofouling_risk_score',\n",
        "    'beaufort', 'seaCondition', 'lat_mean', 'lon_mean',\n",
        "    'temp_proxy', 'temp_risk', 'region_risk',\n",
        "    'days_since_clean', 'fouling_stage',\n",
        "    'displacement'\n",
        "]\n",
        "\n",
        "if 'CONSUMED_QUANTITY' in df_events_ais.columns:\n",
        "    features_v2.append('CONSUMED_QUANTITY')\n",
        "\n",
        "features_available = [f for f in features_v2 if f in df_events_ais.columns]\n",
        "\n",
        "print(f\"\\nüìä Features dispon√≠veis: {len(features_available)}\")\n",
        "\n",
        "df_ml = df_events_ais.dropna(subset=['fouling_rating'])[features_available + ['fouling_rating', 'fouling_label', 'startGMTDate', 'shipName']].copy()\n",
        "df_ml[features_available] = df_ml[features_available].fillna(0)\n",
        "\n",
        "print(f\"‚úÖ Dataset ML: {df_ml.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyekrHB1257o"
      },
      "source": [
        "## 8.  VALIDA√á√ÉO TEMPORAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PePwVBCR257o",
        "outputId": "5f5d0118-820a-45e3-e99e-7fb677a24889",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚è∞ Preparando valida√ß√£o temporal...\n",
            "‚úÖ Treino: 3711 | Teste: 928\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "\n",
        "print(\"\\n‚è∞ Preparando valida√ß√£o temporal...\")\n",
        "df_ml_sorted = df_ml.sort_values('startGMTDate').reset_index(drop=True)\n",
        "\n",
        "X = df_ml_sorted[features_available].values\n",
        "y_reg = df_ml_sorted['fouling_rating'].values\n",
        "y_clf = LabelEncoder().fit_transform(df_ml_sorted['fouling_label'].astype(str).values)\n",
        "\n",
        "split_idx = int(len(df_ml_sorted) * 0.8)\n",
        "\n",
        "X_train = X[:split_idx]\n",
        "X_test = X[split_idx:]\n",
        "y_train = y_reg[:split_idx]\n",
        "y_test = y_reg[split_idx:]\n",
        "\n",
        "print(f\"‚úÖ Treino: {X_train.shape[0]} | Teste: {X_test.shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMQtNFSM257o"
      },
      "source": [
        "## 9. üéØ MODELO ENSEMBLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qiu8V5n4257o",
        "outputId": "2b7660ab-1771-44f3-f01e-077d25221251",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Treinando ensemble...\n",
            "\n",
            "Treinando XGBoost...\n",
            "  MAE: 0.0070 | RMSE: 0.0251 | R¬≤: 0.9992\n",
            "\n",
            "Treinando LightGBM...\n",
            "  MAE: 0.0072 | RMSE: 0.0222 | R¬≤: 0.9994\n",
            "\n",
            "Treinando RandomForest...\n",
            "  MAE: 0.0046 | RMSE: 0.0195 | R¬≤: 0.9995\n",
            "\n",
            "Treinando GradientBoosting...\n",
            "  MAE: 0.0042 | RMSE: 0.0152 | R¬≤: 0.9997\n",
            "\n",
            "============================================================\n",
            " ENSEMBLE:\n",
            "  MAE: 0.0048\n",
            "  RMSE: 0.0170\n",
            "  R¬≤: 0.9996\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "\n",
        "print(\"\\n Treinando ensemble...\")\n",
        "\n",
        "models = {\n",
        "    'XGBoost': xgb.XGBRegressor(\n",
        "        n_estimators=300, learning_rate=0.03, max_depth=6,\n",
        "        subsample=0.8, colsample_bytree=0.8, random_state=42, verbosity=0\n",
        "    ),\n",
        "    'LightGBM': lgb.LGBMRegressor(\n",
        "        n_estimators=300, learning_rate=0.03, max_depth=6,\n",
        "        random_state=42, verbosity=-1\n",
        "    ),\n",
        "    'RandomForest': RandomForestRegressor(\n",
        "        n_estimators=200, max_depth=10, random_state=42, n_jobs=-1\n",
        "    ),\n",
        "    'GradientBoosting': GradientBoostingRegressor(\n",
        "        n_estimators=200, learning_rate=0.05, max_depth=5, random_state=42\n",
        "    )\n",
        "}\n",
        "\n",
        "predictions = {}\n",
        "model_scores = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTreinando {name}...\")\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    predictions[name] = y_pred\n",
        "\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    model_scores[name] = {'MAE': mae, 'RMSE': rmse, 'R2': r2}\n",
        "    print(f\"  MAE: {mae:.4f} | RMSE: {rmse:.4f} | R¬≤: {r2:.4f}\")\n",
        "\n",
        "# Ensemble com pesos\n",
        "maes = [model_scores[name]['MAE'] for name in models.keys()]\n",
        "weights = [1/mae for mae in maes]\n",
        "weights = [w/sum(weights) for w in weights]\n",
        "\n",
        "y_pred_ensemble = sum(predictions[name] * weight for name, weight in zip(models.keys(), weights))\n",
        "\n",
        "mae_ensemble = mean_absolute_error(y_test, y_pred_ensemble)\n",
        "rmse_ensemble = np.sqrt(mean_squared_error(y_test, y_pred_ensemble))\n",
        "r2_ensemble = r2_score(y_test, y_pred_ensemble)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\" ENSEMBLE:\")\n",
        "print(f\"  MAE: {mae_ensemble:.4f}\")\n",
        "print(f\"  RMSE: {rmse_ensemble:.4f}\")\n",
        "print(f\"  R¬≤: {r2_ensemble:.4f}\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxKIAZdp257o"
      },
      "source": [
        "## 10.  IMPACTO ECON√îMICO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzG2vz7q257o",
        "outputId": "a995fdc0-3202-4550-bfb2-e53c30aae076",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Calculando impacto econ√¥mico...\n",
            "Custo Extra M√©dio/Dia: $5,145.31\n",
            "Custo Extra M√©dio/M√™s: $154,359.24\n",
            "Custo Extra M√©dio/Ano: $1,878,037.45\n",
            "CO2 Extra M√©dio/Ano: 8,997.24 tons\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "\n",
        "def compute_fuel_penalty_from_fouling(fouling_rating, baseline_consumption):\n",
        "    \"\"\"Calcula penalidade baseado em IMO (5-25%)\"\"\"\n",
        "    PRICE_PER_TON = 650\n",
        "    CO2_PER_TON = 3.114\n",
        "\n",
        "    penalty_map = {0: 0.00, 1: 0.065, 2: 0.10, 3: 0.15, 4: 0.215}\n",
        "\n",
        "    if fouling_rating <= 0:\n",
        "        penalty = 0\n",
        "    elif fouling_rating >= 4:\n",
        "        penalty = 0.25\n",
        "    else:\n",
        "        lower = int(fouling_rating)\n",
        "        upper = min(lower + 1, 4)\n",
        "        fraction = fouling_rating - lower\n",
        "        penalty = penalty_map[lower] + fraction * (penalty_map[upper] - penalty_map[lower])\n",
        "\n",
        "    extra_fuel = baseline_consumption * penalty\n",
        "\n",
        "    return {\n",
        "        'fouling_rating': fouling_rating,\n",
        "        'fuel_penalty_pct': penalty * 100,\n",
        "        'extra_fuel_tons_day': extra_fuel,\n",
        "        'extra_cost_usd_day': extra_fuel * PRICE_PER_TON,\n",
        "        'extra_cost_usd_month': extra_fuel * PRICE_PER_TON * 30,\n",
        "        'extra_cost_usd_year': extra_fuel * PRICE_PER_TON * 365,\n",
        "        'extra_co2_tons_year': extra_fuel * CO2_PER_TON * 365\n",
        "    }\n",
        "\n",
        "print(\"\\n Calculando impacto econ√¥mico...\")\n",
        "baseline = 40\n",
        "all_impacts = [compute_fuel_penalty_from_fouling(pred, baseline) for pred in y_pred_ensemble]\n",
        "df_impacts = pd.DataFrame(all_impacts)\n",
        "\n",
        "print(f\"Custo Extra M√©dio/Dia: ${df_impacts['extra_cost_usd_day'].mean():,.2f}\")\n",
        "print(f\"Custo Extra M√©dio/M√™s: ${df_impacts['extra_cost_usd_month'].mean():,.2f}\")\n",
        "print(f\"Custo Extra M√©dio/Ano: ${df_impacts['extra_cost_usd_year'].mean():,.2f}\")\n",
        "print(f\"CO2 Extra M√©dio/Ano: {df_impacts['extra_co2_tons_year'].mean():,.2f} tons\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqdU4u_S257o"
      },
      "source": [
        "## 11.  AN√ÅLISE DE CEN√ÅRIOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8HzLEIn257o",
        "outputId": "06f84379-2015-4f04-9a9c-c5dbcc03b0f6",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Simulando cen√°rios...\n",
            "\n",
            "Fouling atual: 4.00\n",
            "\n",
            "N√£o Fazer Nada:\n",
            "  Custo total: $1,088,091.23\n",
            "  Fouling final: 4.00\n",
            "\n",
            "Limpar Agora:\n",
            "  Custo total: $480,050.00\n",
            "  Fouling final: 2.00\n",
            "\n",
            "‚úÖ RECOMENDA√á√ÉO: Limpar Agora\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "\n",
        "def simulate_cleaning_scenarios(current_fouling, days_since_clean, baseline=40):\n",
        "    \"\"\"Simula cen√°rios de limpeza\"\"\"\n",
        "    CLEANING_COST = 50000\n",
        "    DOWNTIME_COST = 24 * 5000\n",
        "    DAYS_AHEAD = 180\n",
        "\n",
        "    scenarios = {}\n",
        "\n",
        "    # Cen√°rio 1: N√£o fazer nada\n",
        "    future_fouling = min(current_fouling + (DAYS_AHEAD / 90), 4.0)\n",
        "    current_impact = compute_fuel_penalty_from_fouling(current_fouling, baseline)\n",
        "    future_impact = compute_fuel_penalty_from_fouling(future_fouling, baseline)\n",
        "    avg_cost = (current_impact['extra_cost_usd_day'] + future_impact['extra_cost_usd_day']) / 2\n",
        "\n",
        "    scenarios['N√£o Fazer Nada'] = {\n",
        "        'total_cost': avg_cost * DAYS_AHEAD,\n",
        "        'final_fouling': future_fouling\n",
        "    }\n",
        "\n",
        "    # Cen√°rio 2: Limpar agora\n",
        "    post_clean = 0.5\n",
        "    future_clean = min(post_clean + (DAYS_AHEAD / 120), 2.5)\n",
        "    post_impact = compute_fuel_penalty_from_fouling(post_clean, baseline)\n",
        "    future_impact_clean = compute_fuel_penalty_from_fouling(future_clean, baseline)\n",
        "    avg_cost_clean = (post_impact['extra_cost_usd_day'] + future_impact_clean['extra_cost_usd_day']) / 2\n",
        "\n",
        "    scenarios['Limpar Agora'] = {\n",
        "        'total_cost': CLEANING_COST + DOWNTIME_COST + (avg_cost_clean * DAYS_AHEAD),\n",
        "        'final_fouling': future_clean\n",
        "    }\n",
        "\n",
        "    return scenarios\n",
        "\n",
        "print(\"\\n Simulando cen√°rios...\")\n",
        "example_fouling = y_pred_ensemble[0]\n",
        "example_days = df_ml_sorted.iloc[split_idx]['days_since_clean']\n",
        "\n",
        "scenarios = simulate_cleaning_scenarios(example_fouling, example_days)\n",
        "\n",
        "print(f\"\\nFouling atual: {example_fouling:.2f}\")\n",
        "for name, data in scenarios.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Custo total: ${data['total_cost']:,.2f}\")\n",
        "    print(f\"  Fouling final: {data['final_fouling']:.2f}\")\n",
        "\n",
        "best = min(scenarios.items(), key=lambda x: x[1]['total_cost'])[0]\n",
        "print(f\"\\n‚úÖ RECOMENDA√á√ÉO: {best}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHftztw6257o"
      },
      "source": [
        "## 12. SALVAR MODELOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leOuNPBz257p",
        "outputId": "4a81ca3f-fd36-426d-e805-b908d50f9025",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Salvando modelos...\n",
            " Modelos salvos\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "\n",
        "print(\"\\n Salvando modelos...\")\n",
        "for name, model in models.items():\n",
        "    filename = f\"model_{name.lower().replace(' ', '_')}_v2.pkl\"\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "metadata = {\n",
        "    'features': features_available,\n",
        "    'weights': dict(zip(models.keys(), weights)),\n",
        "    'mae': mae_ensemble,\n",
        "    'rmse': rmse_ensemble,\n",
        "    'r2': r2_ensemble\n",
        "}\n",
        "\n",
        "with open('model_metadata_v2.pkl', 'wb') as f:\n",
        "    pickle.dump(metadata, f)\n",
        "\n",
        "print(\" Modelos salvos\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "GEZvqncI257p"
      },
      "outputs": [],
      "source": [
        "# 13. RESUMO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWpvCu-A257p",
        "outputId": "64b9d9f1-a883-4e5e-9378-4b5c9aa2e565",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            " RESUMO SOLU√á√ÉO DE PREDI√á√ÉO\n",
            "================================================================================\n",
            "\n",
            " PERFORMANCE:\n",
            "  MAE:  0.0048\n",
            "  RMSE: 0.0170\n",
            "  R¬≤:   0.9996\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" RESUMO SOLU√á√ÉO DE PREDI√á√ÉO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n PERFORMANCE:\")\n",
        "print(f\"  MAE:  {mae_ensemble:.4f}\")\n",
        "print(f\"  RMSE: {rmse_ensemble:.4f}\")\n",
        "print(f\"  R¬≤:   {r2_ensemble:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK6UZe72257p"
      },
      "source": [
        "## 13. FOULING RATING POR NAVIO (FROTA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5o6rMjI257p",
        "outputId": "eb2e53e3-066a-4b0c-88b2-d2081292d4fd",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "üö¢ FOULING RATING POR NAVIO DA FROTA\n",
            "================================================================================\n",
            "\n",
            " ESCALA IMO MEPC.378(80):\n",
            "  0: Sem bioincrusta√ß√£o\n",
            "  1: Microincrusta√ß√£o (biofilme/lodo/limo)\n",
            "  2: Macroincrusta√ß√£o leve (1-15% superf√≠cie)\n",
            "  3: Macroincrusta√ß√£o moderada (16-40% superf√≠cie)\n",
            "  4: Macroincrusta√ß√£o pesada (41-100% superf√≠cie)\n",
            "\n",
            "            Navio  Fouling  Classifica√ß√£o  Dias Limpeza Penalidade Custo/M√™s  Custo/Ano CO2/Ano               A√ß√£o Urg√™ncia\n",
            "       BRUNO LIMA     4.00      4: Pesada           565      25.0%  $195,000 $2,372,500 11,366t  üî¥ Limpeza CR√çTICA        üî¥\n",
            "   DANIEL PEREIRA     4.00      4: Pesada          1739      25.0%  $195,000 $2,372,500 11,366t  üî¥ Limpeza CR√çTICA        üî¥\n",
            "    EDUARDO COSTA     4.00      4: Pesada           316      25.0%  $195,000 $2,372,500 11,366t  üî¥ Limpeza CR√çTICA        üî¥\n",
            "  VICTOR OLIVEIRA     4.00      4: Pesada          1726      25.0%  $195,000 $2,372,500 11,366t  üî¥ Limpeza CR√çTICA        üî¥\n",
            "MARCOS CAVALCANTI     4.00      4: Pesada           666      25.0%  $195,000 $2,372,500 11,366t  üî¥ Limpeza CR√çTICA        üî¥\n",
            "  MARIA VALENTINA     3.99  3-4: Moderada           374      21.4%  $167,044 $2,032,366  9,737t  üî¥ Limpeza Urgente        üî¥\n",
            "     RAUL MARTINS     3.87  3-4: Moderada           300      20.7%  $161,362 $1,963,244  9,405t  üî¥ Limpeza Urgente        üî¥\n",
            " RODRIGO PINHEIRO     3.72  3-4: Moderada           823      19.7%  $153,757 $1,870,716  8,962t  üî¥ Limpeza Urgente        üî¥\n",
            " GISELLE CARVALHO     3.72  3-4: Moderada          1023      19.7%  $153,757 $1,870,716  8,962t  üî¥ Limpeza Urgente        üî¥\n",
            "   FELIPE RIBEIRO     3.72  3-4: Moderada          1014      19.7%  $153,757 $1,870,716  8,962t  üî¥ Limpeza Urgente        üî¥\n",
            "      PAULO MOURA     3.07  3-4: Moderada           105      15.5%  $120,802 $1,469,764  7,041t  üî¥ Limpeza Urgente        üî¥\n",
            "    RAFAEL SANTOS     2.22      2-3: Leve            42      11.1%   $86,775 $1,055,762  5,058t  üü† Limpeza Reativa        üü†\n",
            "   HENRIQUE ALVES     2.07      2-3: Leve            53      10.4%   $80,925   $984,587  4,717t  üü† Limpeza Reativa        üü†\n",
            "      CARLA SILVA     1.97     1-2: Micro            41       9.9%   $77,318   $940,696  4,507t üü° Limpeza Proativa        üü°\n",
            "    ROMARIO SILVA     0.58 0-1: Sem/Micro             9       3.8%   $29,515   $359,095  1,720t               ‚úÖ OK        üü¢\n",
            "\n",
            "================================================================================\n",
            " ESTAT√çSTICAS DA FROTA\n",
            "================================================================================\n",
            "\n",
            " Fouling Rating M√©dio da Frota: 3.26\n",
            " Fouling Rating M√≠nimo: 0.58\n",
            " Fouling Rating M√°ximo: 4.00\n",
            " Desvio Padr√£o: 1.02\n",
            "\n",
            " Distribui√ß√£o por Categoria (Escala IMO):\n",
            "   üü¢ 0-1 (Sem/Micro):       1 navios (  6.7%)\n",
            "   üü° 1-2 (Micro):           1 navios (  6.7%)\n",
            "   üü† 2-3 (Leve):            2 navios ( 13.3%)\n",
            "   üî¥ 3-4 (Moderada):        6 navios ( 40.0%)\n",
            "   üî¥ 4   (Pesada):          5 navios ( 33.3%)\n",
            "\n",
            "‚ö†Ô∏è A√á√ïES REQUERIDAS:\n",
            "   üî¥ 11 navios precisam LIMPEZA URGENTE (Fouling ‚â• 3.0)\n",
            "   üü† 2 navios precisam LIMPEZA REATIVA (Fouling 2.0-3.0)\n",
            "   üü° 1 navios precisam LIMPEZA PROATIVA (Fouling 1.0-2.0)\n",
            "\n",
            " IMPACTO ECON√îMICO TOTAL DA FROTA:\n",
            "   Custo Extra Total/Ano: $26,280,162\n",
            "   Custo Extra M√©dio/Navio: $1,752,011\n",
            "   CO2 Extra Total/Ano: 125,901 toneladas\n",
            "   CO2 Extra M√©dio/Navio: 8,393 toneladas\n",
            "\n",
            " Resultados salvos em: fouling_por_navio.csv\n",
            "\n",
            "================================================================================\n",
            "SCRIPT CONCLU√çDO!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üö¢ FOULING RATING POR NAVIO DA FROTA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Pegar √∫ltimo evento de cada navio\n",
        "df_ml_sorted_final = df_ml_sorted.copy()\n",
        "\n",
        "# Verificar se shipName existe, sen√£o usar √≠ndice\n",
        "if 'shipName' not in df_ml_sorted_final.columns:\n",
        "    # Adicionar shipName do df_ml original\n",
        "    df_ml_sorted_final = df_ml_sorted_final.merge(\n",
        "        df_ml[['startGMTDate', 'shipName']],\n",
        "        on='startGMTDate',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "df_ml_sorted_final['shipName_clean'] = df_ml_sorted_final['shipName'].astype(str).str.strip()\n",
        "\n",
        "# √öltimo evento de cada navio (mais recente)\n",
        "ultimos_eventos = df_ml_sorted_final.groupby('shipName_clean').last().reset_index()\n",
        "\n",
        "# Calcular impacto econ√¥mico para cada navio\n",
        "resultados_frota = []\n",
        "\n",
        "for _, navio_data in ultimos_eventos.iterrows():\n",
        "    ship_name = navio_data['shipName_clean']\n",
        "    fouling = navio_data['fouling_rating']\n",
        "    days_clean = navio_data.get('days_since_clean', np.nan)\n",
        "\n",
        "    # Calcular impacto\n",
        "    baseline = 40  # tons/dia (ajustar se tiver dados espec√≠ficos)\n",
        "    impacto = compute_fuel_penalty_from_fouling(fouling, baseline)\n",
        "\n",
        "    # Classificar por escala IMO\n",
        "    if fouling < 1:\n",
        "        classificacao = \"0-1: Sem/Micro\"\n",
        "        acao = \"‚úÖ OK\"\n",
        "        urgencia = \"üü¢\"\n",
        "    elif fouling < 2:\n",
        "        classificacao = \"1-2: Micro\"\n",
        "        acao = \"üü° Limpeza Proativa\"\n",
        "        urgencia = \"üü°\"\n",
        "    elif fouling < 3:\n",
        "        classificacao = \"2-3: Leve\"\n",
        "        acao = \"üü† Limpeza Reativa\"\n",
        "        urgencia = \"üü†\"\n",
        "    elif fouling < 4:\n",
        "        classificacao = \"3-4: Moderada\"\n",
        "        acao = \"üî¥ Limpeza Urgente\"\n",
        "        urgencia = \"üî¥\"\n",
        "    else:\n",
        "        classificacao = \"4: Pesada\"\n",
        "        acao = \"üî¥ Limpeza CR√çTICA\"\n",
        "        urgencia = \"üî¥\"\n",
        "\n",
        "    resultados_frota.append({\n",
        "        'Navio': ship_name,\n",
        "        'Fouling': round(fouling, 2),\n",
        "        'Classifica√ß√£o': classificacao,\n",
        "        'Dias Limpeza': int(days_clean) if not pd.isna(days_clean) else 'N/A',\n",
        "        'Penalidade': f\"{impacto['fuel_penalty_pct']:.1f}%\",\n",
        "        'Custo/M√™s': f\"${impacto['extra_cost_usd_month']:,.0f}\",\n",
        "        'Custo/Ano': f\"${impacto['extra_cost_usd_year']:,.0f}\",\n",
        "        'CO2/Ano': f\"{impacto['extra_co2_tons_year']:,.0f}t\",\n",
        "        'A√ß√£o': acao,\n",
        "        'Urg√™ncia': urgencia\n",
        "    })\n",
        "\n",
        "df_frota = pd.DataFrame(resultados_frota)\n",
        "\n",
        "# Ordenar por Fouling Rating (maior primeiro)\n",
        "df_frota = df_frota.sort_values('Fouling', ascending=False)\n",
        "\n",
        "print(\"\\n ESCALA IMO MEPC.378(80):\")\n",
        "print(\"  0: Sem bioincrusta√ß√£o\")\n",
        "print(\"  1: Microincrusta√ß√£o (biofilme/lodo/limo)\")\n",
        "print(\"  2: Macroincrusta√ß√£o leve (1-15% superf√≠cie)\")\n",
        "print(\"  3: Macroincrusta√ß√£o moderada (16-40% superf√≠cie)\")\n",
        "print(\"  4: Macroincrusta√ß√£o pesada (41-100% superf√≠cie)\")\n",
        "\n",
        "print(\"\\n\" + df_frota.to_string(index=False))\n",
        "\n",
        "# Estat√≠sticas da frota\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" ESTAT√çSTICAS DA FROTA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "fouling_values = df_frota['Fouling'].values\n",
        "print(f\"\\n Fouling Rating M√©dio da Frota: {fouling_values.mean():.2f}\")\n",
        "print(f\" Fouling Rating M√≠nimo: {fouling_values.min():.2f}\")\n",
        "print(f\" Fouling Rating M√°ximo: {fouling_values.max():.2f}\")\n",
        "print(f\" Desvio Padr√£o: {fouling_values.std():.2f}\")\n",
        "\n",
        "# Distribui√ß√£o por categoria\n",
        "clean_count = (fouling_values < 1).sum()\n",
        "micro_count = ((fouling_values >= 1) & (fouling_values < 2)).sum()\n",
        "leve_count = ((fouling_values >= 2) & (fouling_values < 3)).sum()\n",
        "moderada_count = ((fouling_values >= 3) & (fouling_values < 4)).sum()\n",
        "pesada_count = (fouling_values >= 4).sum()\n",
        "\n",
        "total_navios = len(fouling_values)\n",
        "\n",
        "print(f\"\\n Distribui√ß√£o por Categoria (Escala IMO):\")\n",
        "print(f\"   üü¢ 0-1 (Sem/Micro):      {clean_count:2d} navios ({clean_count/total_navios*100:5.1f}%)\")\n",
        "print(f\"   üü° 1-2 (Micro):          {micro_count:2d} navios ({micro_count/total_navios*100:5.1f}%)\")\n",
        "print(f\"   üü† 2-3 (Leve):           {leve_count:2d} navios ({leve_count/total_navios*100:5.1f}%)\")\n",
        "print(f\"   üî¥ 3-4 (Moderada):       {moderada_count:2d} navios ({moderada_count/total_navios*100:5.1f}%)\")\n",
        "print(f\"   üî¥ 4   (Pesada):         {pesada_count:2d} navios ({pesada_count/total_navios*100:5.1f}%)\")\n",
        "\n",
        "# Prioriza√ß√£o de a√ß√µes\n",
        "urgente_count = (fouling_values >= 3).sum()\n",
        "monitorar_count = ((fouling_values >= 2) & (fouling_values < 3)).sum()\n",
        "proativa_count = ((fouling_values >= 1) & (fouling_values < 2)).sum()\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è A√á√ïES REQUERIDAS:\")\n",
        "print(f\"   üî¥ {urgente_count} navios precisam LIMPEZA URGENTE (Fouling ‚â• 3.0)\")\n",
        "print(f\"   üü† {monitorar_count} navios precisam LIMPEZA REATIVA (Fouling 2.0-3.0)\")\n",
        "print(f\"   üü° {proativa_count} navios precisam LIMPEZA PROATIVA (Fouling 1.0-2.0)\")\n",
        "\n",
        "# Impacto econ√¥mico total\n",
        "custo_ano_values = df_frota['Custo/Ano'].str.replace('$', '').str.replace(',', '').astype(float)\n",
        "co2_ano_values = df_frota['CO2/Ano'].str.replace('t', '').str.replace(',', '').astype(float)\n",
        "\n",
        "total_custo = custo_ano_values.sum()\n",
        "total_co2 = co2_ano_values.sum()\n",
        "\n",
        "print(f\"\\n IMPACTO ECON√îMICO TOTAL DA FROTA:\")\n",
        "print(f\"   Custo Extra Total/Ano: ${total_custo:,.0f}\")\n",
        "print(f\"   Custo Extra M√©dio/Navio: ${total_custo/total_navios:,.0f}\")\n",
        "print(f\"   CO2 Extra Total/Ano: {total_co2:,.0f} toneladas\")\n",
        "print(f\"   CO2 Extra M√©dio/Navio: {total_co2/total_navios:,.0f} toneladas\")\n",
        "\n",
        "# Salvar resultados detalhados\n",
        "df_frota.to_csv('fouling_por_navio.csv', index=False)\n",
        "print(f\"\\n Resultados salvos em: fouling_por_navio.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SCRIPT CONCLU√çDO!\")\n",
        "print(\"=\"*80)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}